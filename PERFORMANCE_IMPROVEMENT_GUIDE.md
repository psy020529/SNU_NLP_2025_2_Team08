# ì„±ëŠ¥ í–¥ìƒ ì‹¤í–‰ ê°€ì´ë“œ

## ğŸ¯ ëª©í‘œ
1. **í”„ë ˆì´ë° ì°¨ì´ ì¦í­**: í˜„ì¬ ë¯¸ë¬˜í•œ ì°¨ì´ë¥¼ ë” ëª…í™•í•˜ê²Œ
2. **íƒì§€ê¸° ì„±ëŠ¥ ê°œì„ **: AUC 0.331 â†’ 0.70+ ëª©í‘œ

---

## ğŸ“‹ ë‹¨ê³„ë³„ ì‹¤í–‰ ì ˆì°¨

### STEP 1: ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ìƒì„± (í•„ìˆ˜) âœ…

**ë³€ê²½ì‚¬í•­**: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì§§ì€ ë¬¸ì¥ì—ì„œ êµ¬ì²´ì ì¸ ì§€ì‹œì‚¬í•­ìœ¼ë¡œ í™•ì¥

**ì‹¤í–‰ ëª…ë ¹ì–´**:
```powershell
# 1-1. NEUTRAL ì¬ìƒì„± (ê°•í™”ëœ í”„ë¡¬í”„íŠ¸)
python -m project.scripts.infer_baseline `
  --frame NEUTRAL `
  --num-samples 30 `
  --output nlp-proj/outputs/baseline_responses/neutral_v2.jsonl `
  --batch-size 2 `
  --device cpu `
  --prompts-file nlp-proj/data/prompts/sample_prompts.jsonl

# 1-2. PRO ì¬ìƒì„±
python -m project.scripts.infer_baseline `
  --frame PRO `
  --num-samples 30 `
  --output nlp-proj/outputs/manipulated_responses/pro_v2.jsonl `
  --batch-size 2 `
  --device cpu `
  --prompts-file nlp-proj/data/prompts/sample_prompts.jsonl

# 1-3. CON ì¬ìƒì„±
python -m project.scripts.infer_baseline `
  --frame CON `
  --num-samples 30 `
  --output nlp-proj/outputs/manipulated_responses/con_v2.jsonl `
  --batch-size 2 `
  --device cpu `
  --prompts-file nlp-proj/data/prompts/sample_prompts.jsonl
```

**ì˜ˆìƒ ì‹œê°„**: ê° 30ë¶„ (ì´ 1.5ì‹œê°„)

**ê¸°ëŒ€ íš¨ê³¼**:
- í”„ë ˆì„ ê°„ sentiment ì°¨ì´ ì¦ê°€
- ì‘ë‹µ ìŠ¤íƒ€ì¼ ì°¨ë³„í™” (PRO: ë‚™ê´€ì , CON: ë¹„íŒì )

---

### STEP 2: ì ìˆ˜ ì¬ê³„ì‚°

```powershell
python -m project.scripts.score_responses `
  --inputs nlp-proj/outputs/baseline_responses/neutral_v2.jsonl `
           nlp-proj/outputs/manipulated_responses/pro_v2.jsonl `
           nlp-proj/outputs/manipulated_responses/con_v2.jsonl `
  --out_csv nlp-proj/outputs/scores/scores_v2.csv `
  --method vader
```

**í™•ì¸ì‚¬í•­**:
- PROì˜ polarityê°€ 0.3 ì´ìƒ (í˜„ì¬ -0.085)
- CONì˜ polarityê°€ -0.5 ì´í•˜ (í˜„ì¬ -0.162)
- í”„ë ˆì„ ê°„ ì°¨ì´ê°€ 0.5 ì´ìƒ

---

### STEP 3: í–¥ìƒëœ íƒì§€ê¸° í›ˆë ¨ (í•„ìˆ˜) âœ…

**ë³€ê²½ì‚¬í•­**:
- TF-IDF â†’ 13ê°€ì§€ ì–¸ì–´í•™ì  íŠ¹ì§• (ê°ì • ì–´íœ˜, í™•ì‹ ë„, ë¬¸ì²´ ë“±)
- Logistic Regression â†’ Random Forest (ê³¼ì í•© ë°©ì§€)
- Class weighting ì ìš© (ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬)

**ì‹¤í–‰ ëª…ë ¹ì–´**:
```powershell
# 3-1. ì–¸ì–´í•™ì  íŠ¹ì§•ë§Œ ì‚¬ìš©
python -m project.scripts.train_detector_enhanced `
  --baseline-dir nlp-proj/outputs/baseline_responses `
  --manipulated-dir nlp-proj/outputs/manipulated_responses `
  --output-dir nlp-proj/outputs/detector_results_enhanced

# 3-2. TF-IDF + ì–¸ì–´í•™ì  íŠ¹ì§• ì¡°í•© (ë” ê°•ë ¥)
python -m project.scripts.train_detector_enhanced `
  --baseline-dir nlp-proj/outputs/baseline_responses `
  --manipulated-dir nlp-proj/outputs/manipulated_responses `
  --output-dir nlp-proj/outputs/detector_results_combined `
  --use-tfidf
```

**ê¸°ëŒ€ ì„±ëŠ¥**:
- AUC: 0.331 â†’ 0.60~0.75
- F1 Score: 0.00 (Baseline) â†’ 0.50+
- Confusion Matrix: ë” ê· í˜•ì¡íŒ ì˜ˆì¸¡

---

### STEP 4: Few-shot í•™ìŠµ (ì„ íƒ, ê³ ê¸‰)

**ì‹¤í–‰ ì „ ì¤€ë¹„**: `infer_baseline.py` ìˆ˜ì • í•„ìš”

```python
# project/scripts/infer_baseline.py ì—ì„œ
from project.src.prompts import make_system_prompt
from project.src.prompts_fewshot import make_fewshot_prompt

# ê¸°ì¡´ ë¼ì¸ êµì²´:
# full_prompts = [f"{system_prompt}\n\nQuestion: {p}\n\nAnswer:" for p in prompts]
full_prompts = [make_fewshot_prompt(args.frame, p, system_prompt) for p in prompts]
```

**ì‹¤í–‰**:
```powershell
python -m project.scripts.infer_baseline `
  --frame PRO `
  --num-samples 30 `
  --output nlp-proj/outputs/manipulated_responses/pro_fewshot.jsonl `
  --batch-size 1 `  # Few-shotì€ ì…ë ¥ì´ ê¸¸ì–´ì„œ batch_size=1 ê¶Œì¥
  --device cpu `
  --prompts-file nlp-proj/data/prompts/sample_prompts.jsonl `
  --max-new-tokens 300  # ë” ê¸´ ì‘ë‹µ í—ˆìš©
```

**ì£¼ì˜ì‚¬í•­**:
- Few-shotì€ ì…ë ¥ í† í°ì´ ê¸¸ì–´ì ¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€
- batch_size=1ë¡œ ì¤„ì´ê³  max_new_tokens ëŠ˜ë¦¼

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Before (í˜„ì¬)
```
í”„ë ˆì„ë³„ Polarity:
- CON: -0.162
- NEUTRAL: -0.185
- PRO: -0.085
ì°¨ì´: 0.10 (ë„ˆë¬´ ì‘ìŒ)

íƒì§€ê¸°:
- AUC: 0.331 (ë¬´ì‘ìœ„ë³´ë‹¤ ë‚˜ì¨)
- Baseline F1: 0.00 (ì „í˜€ ëª» ì¡ìŒ)
```

### After (ëª©í‘œ)
```
í”„ë ˆì„ë³„ Polarity:
- CON: -0.50 ì´í•˜
- NEUTRAL: -0.10 ~ 0.10
- PRO: +0.30 ì´ìƒ
ì°¨ì´: 0.80+ (ëª…í™•í•œ êµ¬ë¶„)

íƒì§€ê¸°:
- AUC: 0.70+ (ì‹¤ìš©ì  ìˆ˜ì¤€)
- Baseline F1: 0.50+ (ê· í˜•ì¡íŒ íƒì§€)
- Feature Importance: sentiment_diff, certainty_ratio ìƒìœ„
```

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ (ê¶Œì¥ ìˆœì„œ)

```powershell
# 1. ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ë¡œ NEUTRAL ì¬ìƒì„±
python -m project.scripts.infer_baseline --frame NEUTRAL --num-samples 30 --output nlp-proj/outputs/baseline_responses/neutral_v2.jsonl --batch-size 2 --device cpu --prompts-file nlp-proj/data/prompts/sample_prompts.jsonl

# 2. PRO, CONë„ ì¬ìƒì„± (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ê°€ëŠ¥)
# (ìœ„ STEP 1 ëª…ë ¹ì–´ ì°¸ê³ )

# 3. ì ìˆ˜ ì¬ê³„ì‚° í›„ í™•ì¸
python -m project.scripts.score_responses --inputs nlp-proj/outputs/baseline_responses/neutral_v2.jsonl nlp-proj/outputs/manipulated_responses/pro_v2.jsonl nlp-proj/outputs/manipulated_responses/con_v2.jsonl --out_csv nlp-proj/outputs/scores/scores_v2.csv

# 4. í–¥ìƒëœ íƒì§€ê¸° í›ˆë ¨
python -m project.scripts.train_detector_enhanced --baseline-dir nlp-proj/outputs/baseline_responses --manipulated-dir nlp-proj/outputs/manipulated_responses --output-dir nlp-proj/outputs/detector_results_enhanced --use-tfidf
```

---

## ğŸ“ˆ ì¶”ê°€ ê°œì„  ë°©ì•ˆ (ì‹œê°„ ìˆì„ ê²½ìš°)

### 5. ë” í° ëª¨ë¸ ì‚¬ìš©
```powershell
# flan-t5-base â†’ flan-t5-largeë¡œ ë³€ê²½
python -m project.scripts.infer_baseline `
  --model-name google/flan-t5-large `
  --frame PRO `
  --num-samples 10 `
  --output nlp-proj/outputs/test_large_model.jsonl `
  --batch-size 1 `
  --device cpu
```
**ì£¼ì˜**: flan-t5-largeëŠ” ë©”ëª¨ë¦¬ 3GB+ í•„ìš”

### 6. Temperature ì¡°ì •
```python
# project/src/model_utils.pyì˜ generate_responsesì—ì„œ:
outputs = model.generate(
    **inputs,
    max_new_tokens=max_new_tokens,
    do_sample=True,          # ì¶”ê°€
    temperature=0.7,         # ì¶”ê°€ (0.7-0.9 ê¶Œì¥)
    top_p=0.9,              # ì¶”ê°€
    num_beams=1
)
```

### 7. LoRA íŒŒì¸íŠœë‹ (ê³ ê¸‰, ì„ íƒ)
```powershell
# PRO í”„ë ˆì„ í•™ìŠµìš© í•©ì„± ë°ì´í„° ìƒì„± í›„
python -m project.scripts.run_lora_train `
  --base-model google/flan-t5-base `
  --train-data nlp-proj/data/pro_training_pairs.jsonl `
  --output-dir nlp-proj/lora_models/pro_lora `
  --num-epochs 3 `
  --batch-size 4
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ**: `--batch-size 1`, `--max-new-tokens 128` ë¡œ ì¤„ì´ê¸°
2. **GPU ì—†ì„ ë•Œ**: `--device cpu` í•„ìˆ˜, ì‹œê°„ì€ ì˜¤ë˜ ê±¸ë¦¼ (30ë¶„~1ì‹œê°„)
3. **íŒŒì¼ëª… ì¶©ëŒ**: v2 ì‘ë‹µì€ ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ì§€ ì•Šë„ë¡ ë³„ë„ íŒŒì¼ëª… ì‚¬ìš©
4. **ë””ìŠ¤í¬ ê³µê°„**: ëª¨ë¸ ìºì‹œ(~1GB), ì‘ë‹µ íŒŒì¼(~10MB) í™•ë³´

---

## ğŸ“ ë³´ê³ ì„œ ì‘ì„± íŒ

### ì‹¤í—˜ ì„¹ì…˜ êµ¬ì„±
1. **Baseline (í˜„ì¬ ê²°ê³¼)**: "ì´ˆê¸° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” ë‹¨ìˆœí–ˆìœ¼ë©°..."
2. **Method 1 (ê°•í™”ëœ í”„ë¡¬í”„íŠ¸)**: "êµ¬ì²´ì  ì§€ì‹œì‚¬í•­ì„ ì¶”ê°€í•˜ì—¬..."
3. **Method 2 (í–¥ìƒëœ íŠ¹ì§•)**: "13ê°€ì§€ ì–¸ì–´í•™ì  íŠ¹ì§•ì„ í™œìš©í•˜ì—¬..."
4. **ë¹„êµ ë¶„ì„**: Before/After í‘œ, ê·¸ë˜í”„

### Figure ì¶”ì²œ
- Figure 1: Polarity ë¶„í¬ boxplot (v1 vs v2)
- Figure 2: Confusion matrix ë¹„êµ (simple vs enhanced)
- Figure 3: Feature importance bar chart
- Figure 4: ROC curves ê²¹ì³ì„œ í‘œì‹œ

### Table ì¶”ì²œ
- Table 1: í”„ë ˆì„ë³„ ì ìˆ˜ ë¹„êµ (length, TTR, polarity, sentiment_diff)
- Table 2: íƒì§€ê¸° ì„±ëŠ¥ ë¹„êµ (Acc, Precision, Recall, F1, AUC)
- Table 3: Top 10 ì¤‘ìš” íŠ¹ì§• ë° ì¤‘ìš”ë„ ì ìˆ˜
